{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_dir = /home/matko/Desktop/mozgalo/reports-simple\n",
      "walk_dir (absolute) = /home/matko/Desktop/mozgalo/reports-simple\n",
      "4\n",
      "Done: 0 out of 4\n",
      "/home/matko/Desktop/mozgalo/reports-simple/00/0d17380c8c6e86fbfd1a3cc381e3bf898fa16700.json\n",
      "/home/matko/Desktop/mozgalo/reports-simple/00/0b331dd33f4d70780b0be554c7cdd7ba029ee000.json\n",
      "Done: 2 out of 4\n",
      "/home/matko/Desktop/mozgalo/reports-simple/0a/0d6d94a2a4d5f64e2bfbd188326e8320f608100a.json\n",
      "/home/matko/Desktop/mozgalo/reports-simple/0a/0acca648daf940e57b1daea9296aa11be7b4480a.json\n",
      "Done: 4 out of 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "def translate_to_csv(file_path):\n",
    "    print(file_path)\n",
    "    with open(file_path) as data_file:\n",
    "        data = ujson.load(data_file)\n",
    "\n",
    "    current_dir_path = dirname(os.path.abspath(file_path))\n",
    "    results_dir_path = os.path.join(dirname(dirname(dirname(os.path.abspath(file_path)))), 'reports-csv',\n",
    "                                    current_dir_path.split('/')[-1])\n",
    "    if not os.path.exists(results_dir_path):\n",
    "        os.makedirs(results_dir_path)\n",
    "    filename = os.path.splitext(file_path.split('/')[-1])[0] + '.csv'\n",
    "    result_file_path = os.path.join(results_dir_path, filename)\n",
    "    # print('results path %s ' % result_file_path)\n",
    "\n",
    "    df = json_normalize(data['coreReport']['entries']['entry_list'])\n",
    "    df.to_csv(result_file_path, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    walk_dir = '/home/matko/Desktop/mozgalo/reports-simple'\n",
    "    print('walk_dir = ' + walk_dir)\n",
    "\n",
    "    # If your current working directory may change during script execution, it's recommended to\n",
    "    # immediately convert program arguments to an absolute path. Then the variable root below will\n",
    "    # be an absolute path as well. Example:\n",
    "    # walk_dir = os.path.abspath(walk_dir)\n",
    "    print('walk_dir (absolute) = ' + os.path.abspath(walk_dir))\n",
    "    total_files_count = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            total_files_count += 1\n",
    "    print(total_files_count)\n",
    "    current_files_count = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "\n",
    "        file_paths = []\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_paths.append(file_path)   \n",
    "        with multiprocessing.Pool(processes=4) as pool:\n",
    "            listtt = pool.map(partial(translate_to_csv), file_paths)\n",
    "        pool.join()\n",
    "        current_files_count += len(file_paths)\n",
    "        print('Done: %s out of %s' % (current_files_count, total_files_count))\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]}\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "foo = {1: []}\n",
    "\n",
    "def f(x):\n",
    "    return x\n",
    "\n",
    "def main():\n",
    "    pool = Pool()\n",
    "    foo[1] = pool.map(f, range(100))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print (foo)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_dir = /home/matko/Desktop/mozgalo/reports\n",
      "walk_dir (absolute) = /home/matko/Desktop/mozgalo/reports\n",
      "43784\n",
      "Done: 0 out of 43784\n",
      "--- 0.16091704368591309 seconds ---\n",
      "Done: 173 out of 43784\n",
      "--- 3.2243494987487793 seconds ---\n",
      "Done: 353 out of 43784\n",
      "--- 7.487380027770996 seconds ---\n",
      "Done: 505 out of 43784\n",
      "--- 9.854609966278076 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-808:\n",
      "Process ForkPoolWorker-807:\n",
      "Process ForkPoolWorker-806:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-805:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-224-b146f41ac14e>\", line 31, in translate_to_csv\n",
      "    df = json_normalize(data['coreReport']['entries']['entry_list'])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/pandas/io/json/normalize.py\", line 200, in json_normalize\n",
      "    data = nested_to_record(data, sep=sep)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/pandas/io/json/normalize.py\", line 86, in nested_to_record\n",
      "    new_d.update(nested_to_record(v, newkey, sep, level + 1))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/pandas/io/json/normalize.py\", line 86, in nested_to_record\n",
      "    new_d.update(nested_to_record(v, newkey, sep, level + 1))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-b146f41ac14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-224-b146f41ac14e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mfile_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mlisttt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_to_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcurrent_files_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import multiprocessing\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "def translate_to_csv(file_path):\n",
    "    #print(file_path)\n",
    "    with open(file_path) as data_file:\n",
    "        data = ujson.load(data_file)\n",
    "\n",
    "    current_dir_path = dirname(os.path.abspath(file_path))\n",
    "    results_dir_path = os.path.join(dirname(dirname(dirname(os.path.abspath(file_path)))), 'reports-csv',\n",
    "                                    current_dir_path.split('/')[-1])\n",
    "    if not os.path.exists(results_dir_path):\n",
    "        try:\n",
    "            os.makedirs(results_dir_path)\n",
    "        except FileExistsError as fee:\n",
    "            print(fee)\n",
    "            \n",
    "    filename = os.path.splitext(file_path.split('/')[-1])[0] + '.csv'\n",
    "    result_file_path = os.path.join(results_dir_path, filename)\n",
    "    # print('results path %s ' % result_file_path)\n",
    "\n",
    "    df = json_normalize(data['coreReport']['entries']['entry_list'])\n",
    "    df.to_csv(result_file_path, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    walk_dir = '/home/matko/Desktop/mozgalo/reports'\n",
    "    print('walk_dir = ' + walk_dir)\n",
    "\n",
    "    # If your current working directory may change during script execution, it's recommended to\n",
    "    # immediately convert program arguments to an absolute path. Then the variable root below will\n",
    "    # be an absolute path as well. Example:\n",
    "    # walk_dir = os.path.abspath(walk_dir)\n",
    "    print('walk_dir (absolute) = ' + os.path.abspath(walk_dir))\n",
    "    total_files_count = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            total_files_count += 1\n",
    "    print(total_files_count)\n",
    "    current_files_count = 0\n",
    "    start_time = time.time()\n",
    "    for root, subdirs, files in os.walk(walk_dir):        \n",
    "        file_paths = []\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_paths.append(file_path)   \n",
    "        with multiprocessing.Pool(processes=4) as pool:\n",
    "            listtt = pool.map(partial(translate_to_csv), file_paths)\n",
    "        pool.join()\n",
    "        current_files_count += len(file_paths)\n",
    "        print('Done: %s out of %s' % (current_files_count, total_files_count))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_dir = /home/matko/Desktop/mozgalo/reports-simple\n",
      "walk_dir (absolute) = /home/matko/Desktop/mozgalo/reports-simple\n",
      "6\n",
      "Done: 0 out of 6\n",
      "--- 0.1269826889038086 seconds ---\n",
      "Done: 2 out of 6\n",
      "--- 0.2569611072540283 seconds ---\n",
      "Done: 4 out of 6\n",
      "--- 0.3906381130218506 seconds ---\n",
      "Done: 6 out of 6\n",
      "--- 0.5246031284332275 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import multiprocessing\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "def translate_to_csv(file_path):\n",
    "    #print(file_path)\n",
    "    with open(file_path) as data_file:\n",
    "        data = ujson.load(data_file)\n",
    "\n",
    "    current_dir_path = dirname(os.path.abspath(file_path))\n",
    "    results_dir_path = os.path.join(dirname(dirname(dirname(os.path.abspath(file_path)))), 'reports-csv-simple',\n",
    "                                    current_dir_path.split('/')[-1])\n",
    "    if not os.path.exists(results_dir_path):\n",
    "        try:\n",
    "            os.makedirs(results_dir_path)\n",
    "        except FileExistsError as fee:\n",
    "            print(fee)\n",
    "\n",
    "    filename = os.path.splitext(file_path.split('/')[-1])[0] + '.csv'\n",
    "    result_file_path = os.path.join(results_dir_path, filename)\n",
    "    # print('results path %s ' % result_file_path)\n",
    "\n",
    "    df = json_normalize(data['coreReport']['entries']['entry_list'])\n",
    "    df.to_csv(result_file_path, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    walk_dir = '/home/matko/Desktop/mozgalo/reports-simple'\n",
    "    print('walk_dir = ' + walk_dir)\n",
    "\n",
    "    # If your current working directory may change during script execution, it's recommended to\n",
    "    # immediately convert program arguments to an absolute path. Then the variable root below will\n",
    "    # be an absolute path as well. Example:\n",
    "    # walk_dir = os.path.abspath(walk_dir)\n",
    "    print('walk_dir (absolute) = ' + os.path.abspath(walk_dir))\n",
    "    total_files_count = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            total_files_count += 1\n",
    "    print(total_files_count)\n",
    "    current_files_count = 0\n",
    "    start_time = time.time()\n",
    "    for root, subdirs, files in os.walk(walk_dir):        \n",
    "        file_paths = []\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_paths.append(file_path)   \n",
    "        with multiprocessing.Pool(processes=4) as pool:\n",
    "            listtt = pool.map(partial(translate_to_csv), file_paths)\n",
    "        pool.join()\n",
    "        current_files_count += len(file_paths)\n",
    "        print('Done: %s out of %s' % (current_files_count, total_files_count))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shas = list(sha_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_dir = /home/matko/Desktop/mozgalo/reports-simple\n",
      "walk_dir (absolute) = /home/matko/Desktop/mozgalo/reports-simple\n",
      "6\n",
      "Done: 0 out of 6\n",
      "--- 0.13312721252441406 seconds ---\n",
      "                                                                                          0  \\\n",
      "0d17380c8c6e86fbfd1a3cc381e3bf898fa16700  {'section_names': [['.text', 'RTFOUT_P', '.dat...   \n",
      "\n",
      "                                                        1  \n",
      "0d17380c8c6e86fbfd1a3cc381e3bf898fa16700  {'entropy': 12}  \n",
      "                                                                                          0  \\\n",
      "0b331dd33f4d70780b0be554c7cdd7ba029ee000  {'section_names': [['.ap0x', 'wew8c5we', '.ap0...   \n",
      "\n",
      "                                                        1  \n",
      "0b331dd33f4d70780b0be554c7cdd7ba029ee000  {'entropy': 12}  \n",
      "Done: 2 out of 6\n",
      "--- 0.2685098648071289 seconds ---\n",
      "                                                                                          0  \\\n",
      "0ded3d2726d2e397b6c1fdc26303c0818cc5fc2d  {'section_names': [['.text', 'NO_NAME', '.data...   \n",
      "\n",
      "                                                        1  \n",
      "0ded3d2726d2e397b6c1fdc26303c0818cc5fc2d  {'entropy': 12}                                                                         0  \\\n",
      "00c9a60ecc3d1b7c5647f4b6c89fc41eeaec7e2d  {'section_names': [['.rsrc']]}   \n",
      "\n",
      "                                                        1  \n",
      "00c9a60ecc3d1b7c5647f4b6c89fc41eeaec7e2d  {'entropy': 12}  \n",
      "\n",
      "Done: 4 out of 6\n",
      "--- 0.5042414665222168 seconds ---\n",
      "                                                                                         0  \\\n",
      "0acca648daf940e57b1daea9296aa11be7b4480a  {'section_names': [['CODE', '.rsrc', '.reloc']]}   \n",
      "\n",
      "                                                        1  \n",
      "0acca648daf940e57b1daea9296aa11be7b4480a  {'entropy': 12}                                                                                         0  \\\n",
      "0d6d94a2a4d5f64e2bfbd188326e8320f608100a  {'section_names': [['UPX0', 'UPX1', '.rsrc']]}   \n",
      "\n",
      "                                                        1  \n",
      "0d6d94a2a4d5f64e2bfbd188326e8320f608100a  {'entropy': 12}  \n",
      "\n",
      "Done: 6 out of 6\n",
      "--- 0.6350212097167969 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas\n",
    "import multiprocessing\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "def translate_to_csv(file_path):\n",
    "    #print(file_path)\n",
    "    with open(file_path) as data_file:\n",
    "        data = ujson.load(data_file)\n",
    "\n",
    "    current_dir_path = dirname(os.path.abspath(file_path))\n",
    "    results_dir_path = os.path.join(dirname(dirname(dirname(os.path.abspath(file_path)))), 'reports-csv-simple',\n",
    "                                    current_dir_path.split('/')[-1])\n",
    "    if not os.path.exists(results_dir_path):\n",
    "        try:\n",
    "            os.makedirs(results_dir_path)\n",
    "        except FileExistsError as fee:\n",
    "            print(fee)\n",
    "\n",
    "    filename = os.path.splitext(file_path.split('/')[-1])[0] + '.csv'\n",
    "    result_file_path = os.path.join(results_dir_path, filename)\n",
    "    # print('results path %s ' % result_file_path)\n",
    "    \n",
    "    entry = data['coreReport']['entries']['entry_list'][0]\n",
    "    section_names = []\n",
    "    # It is possible sections do not exist!\n",
    "    if 'sections' not in entry['metadata']['application']['pe']:\n",
    "        print(sha)\n",
    "        section_names.append([])\n",
    "    else:\n",
    "        section_names.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['sections']['section_list']])\n",
    "    #print(section_names)\n",
    "    #section_names\n",
    "    section_names_col = {}\n",
    "    section_names_col['section_names'] = section_names\n",
    "    entropy_col = {}\n",
    "    entropy_col['entropy'] = 12\n",
    "    one_line = {}\n",
    "    one_line[entry['info']['file']['fileName']] = section_names_col, entropy_col\n",
    "    df1 = pandas.DataFrame.from_dict(data=one_line, orient='index')\n",
    "    print(df1)\n",
    "    df = json_normalize(data['coreReport']['entries']['entry_list'])\n",
    "    df1.to_csv(result_file_path, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    walk_dir = '/home/matko/Desktop/mozgalo/reports-simple'\n",
    "    print('walk_dir = ' + walk_dir)\n",
    "\n",
    "    # If your current working directory may change during script execution, it's recommended to\n",
    "    # immediately convert program arguments to an absolute path. Then the variable root below will\n",
    "    # be an absolute path as well. Example:\n",
    "    # walk_dir = os.path.abspath(walk_dir)\n",
    "    print('walk_dir (absolute) = ' + os.path.abspath(walk_dir))\n",
    "    total_files_count = 0\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        for filename in files:\n",
    "            total_files_count += 1\n",
    "    print(total_files_count)\n",
    "    current_files_count = 0\n",
    "    start_time = time.time()\n",
    "    for root, subdirs, files in os.walk(walk_dir):        \n",
    "        file_paths = []\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            file_paths.append(file_path)   \n",
    "        with multiprocessing.Pool(processes=4) as pool:\n",
    "            listtt = pool.map(partial(translate_to_csv), file_paths)\n",
    "        pool.join()\n",
    "        current_files_count += len(file_paths)\n",
    "        print('Done: %s out of %s' % (current_files_count, total_files_count))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43784/43784 [02:45<00:00, 264.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import numpy\n",
    "\n",
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}\n",
    "shas = list(sha_label.keys())\n",
    "# Extracting section names from TC report\n",
    "section_names = []\n",
    "entropies = []\n",
    "children_entropies_mean = []\n",
    "imports = []\n",
    "apis = []\n",
    "rva_difference = []\n",
    "has_children = []\n",
    "imports_count = []\n",
    "apis_count = []\n",
    "children_entropies_std = []\n",
    "\n",
    "text_entropy = []\n",
    "data_entropy = []\n",
    "rdata_entropy = []\n",
    "buildid_entropy = []\n",
    "pdata_entropy = []\n",
    "xdata_entropy = []\n",
    "bss_entropy = []\n",
    "edata_entropy = []\n",
    "idata_entropy = []\n",
    "reloc_entropy = []\n",
    "other_entropy_mean = []\n",
    "no_name_entropy_mean = []\n",
    "\n",
    "other_entropy_std = []\n",
    "no_name_entropy_std = []\n",
    "\n",
    "tags = []\n",
    "checksum_valid = []\n",
    "entry_points_addresses = []\n",
    "image_bases = []\n",
    "image_sizes = []\n",
    "\n",
    "children_tags = [] #All tags of all the children\n",
    "# All sections of all children combined \n",
    "children_sections_entropies_mean = []\n",
    "children_sections_entropies_std = []\n",
    "\n",
    "for sha in tqdm(shas):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    \n",
    "    entropies.append(entry['info']['file']['entropy'])\n",
    "\n",
    "    # It is possible sections do not exist!\n",
    "    if 'sections' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        section_names.append(None)\n",
    "    else:\n",
    "        sec_names = []\n",
    "        no_name_entropies = []\n",
    "        other_entropies = []\n",
    "        # First we presume all are None\n",
    "        text_entropy.append(None)\n",
    "        data_entropy.append(None)\n",
    "        rdata_entropy.append(None)\n",
    "        buildid_entropy.append(None)\n",
    "        pdata_entropy.append(None)\n",
    "        xdata_entropy.append(None)\n",
    "        bss_entropy.append(None)\n",
    "        edata_entropy.append(None)\n",
    "        idata_entropy.append(None)\n",
    "        reloc_entropy.append(None)\n",
    "        for sec in entry['metadata']['application']['pe']['sections']['section_list']:\n",
    "            sec_names.append(sec.get('name', 'NO_NAME'))          \n",
    "\n",
    "            # Now we set only one to Not None\n",
    "            sec_entropy = (sec.get('entropy', None))\n",
    "            if sec_names[-1] == '.text':\n",
    "                text_entropy.pop()\n",
    "                text_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.data':\n",
    "                data_entropy.pop()\n",
    "                data_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.rdata':\n",
    "                rdata_entropy.pop()\n",
    "                rdata_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.buildid':\n",
    "                buildid_entropy.pop()\n",
    "                buildid_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.pdata':\n",
    "                pdata_entropy.pop()\n",
    "                pdata_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.xdata':\n",
    "                xdata_entropy.pop()\n",
    "                xdata_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.bss':\n",
    "                bss_entropy.pop()\n",
    "                bss_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.edata':\n",
    "                edata_entropy.pop()\n",
    "                edata_entropy.append(sec_entropy)\n",
    "            elif sec_names[-1] == '.idata':\n",
    "                idata_entropy.pop()\n",
    "                idata_entropy.append(sec_entropy)                \n",
    "            elif sec_names[-1] == '.reloc':\n",
    "                reloc_entropy.pop()\n",
    "                reloc_entropy.append(sec_entropy)           \n",
    "            elif sec_names[-1] == 'NO_NAME':\n",
    "                no_name_entropies.append(sec_entropy)  \n",
    "            else:\n",
    "                other_entropies.append(sec_entropy)\n",
    "        \n",
    "        if no_name_entropies:\n",
    "            no_name_entropies_numpy_array = numpy.array(no_name_entropies).astype(numpy.float)\n",
    "            no_name_entropy_mean.append(numpy.mean(no_name_entropies_numpy_array, axis=0))\n",
    "            no_name_entropy_std.append(numpy.std(no_name_entropies_numpy_array, axis=0))\n",
    "        else:\n",
    "            no_name_entropy_mean.append(None)\n",
    "            no_name_entropy_std.append(None)\n",
    "            \n",
    "        if other_entropies:\n",
    "            other_entropies_numpy_array = numpy.array(other_entropies).astype(numpy.float)\n",
    "            other_entropy_mean.append(numpy.mean(other_entropies_numpy_array, axis=0))\n",
    "            other_entropy_std.append(numpy.std(other_entropies_numpy_array, axis=0))\n",
    "        else:\n",
    "            other_entropy_mean.append(None)\n",
    "            other_entropy_std.append(None)\n",
    "        \n",
    "        section_names.append(sec_names)\n",
    "        #section_names.append(\n",
    "        #    [sec.get('name', 'NO_NAME')\n",
    "        #     for sec in entry['metadata']['application']['pe']['sections']['section_list']])\n",
    "        \n",
    "    children_sections_entropies_list = []\n",
    "    children_tags_list = []\n",
    "    children_entries = report['coreReport']['entries']['entry_list'][1:]\n",
    "    if children_entries:\n",
    "        has_children.append(True)\n",
    "        #children_tags.extend([child_entry['tags']['tag_list'] for child_entry in children_entries])\n",
    "        #entropy_list = [float(child_entry['info']['file']['entropy']) for child_entry in children_entries]\n",
    "        #print (entropy_list)\n",
    "        #children_entropies.append(reduce(lambda x, y: x + y, entropy_list) / float(len(entropy_list))) \n",
    "        \n",
    "        entropy_list = []\n",
    "\n",
    "        for child_entry in children_entries:\n",
    "            entropy_list.append(float(child_entry['info']['file']['entropy']))\n",
    "            if 'tags' in child_entry and child_entry['tags']:                \n",
    "                children_tags_list.extend(child_entry['tags']['tag_list'])\n",
    "            else:\n",
    "                children_tags_list.append('NO_TAGS')            \n",
    "            if child_entry['metadata'] and 'application' not in child_entry['metadata'] and 'sections' not in child_entry['metadata']['application']['pe']:\n",
    "                continue\n",
    "            else:\n",
    "                one_child_sections_entropies = [float(sec['entropy']) for sec in entry['metadata']['application']['pe']['sections']['section_list']]\n",
    "                children_sections_entropies_list.extend(one_child_sections_entropies)\n",
    "         \n",
    "        entropy_numpy_array = numpy.array(entropy_list)\n",
    "        children_entropies_mean.append(numpy.mean(entropy_numpy_array, axis=0))\n",
    "        children_entropies_std.append(numpy.std(entropy_numpy_array, axis=0))\n",
    "        \n",
    "    else:\n",
    "        children_entropies_mean.append(None)\n",
    "        children_entropies_std.append(None)\n",
    "        has_children.append(False)\n",
    "        \n",
    "    if children_tags_list:\n",
    "        children_tags.append(children_tags_list)    \n",
    "    else:\n",
    "        children_tags.append(['NO_TAGS'])\n",
    "        \n",
    "    if children_sections_entropies_list:\n",
    "        children_sections_entropies_array = numpy.array(children_sections_entropies_list)\n",
    "        children_sections_entropies_mean.append(numpy.mean(children_sections_entropies_array, axis=0))\n",
    "        children_sections_entropies_std.append(numpy.std(children_sections_entropies_array, axis=0))\n",
    "    else:\n",
    "        children_sections_entropies_mean.append(None)\n",
    "        children_sections_entropies_std.append(None)\n",
    "\n",
    "    # It is possible sections do not exist!\n",
    "    if 'imports' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        imports.append(\"NONE\")\n",
    "        apis.append(\"NONE\")\n",
    "        imports_count.append(0)\n",
    "        apis_count.append(0)\n",
    "    else:\n",
    "        imports.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']])\n",
    "        apiss = [sec.get('api_list', 'NO_API_LIST')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']]\n",
    "        apis.append(list(itertools.chain.from_iterable(apiss)))\n",
    "        imports_count.append(len(imports[-1]))\n",
    "        apis_count.append(len(apis[-1]))\n",
    "    \n",
    "    tags.append([tag for tag in entry['tags']['tag_list']])\n",
    "    checksum_valid.append(entry['metadata']['application']['pe']['optionalHeader']['isChecksumValid'])\n",
    "    entry_points_addresses.append(int(entry['metadata']['application']['pe']['optionalHeader']['addressOfEntryPoint'], 16))\n",
    "    image_bases.append(int(entry['metadata']['application']['pe']['optionalHeader']['imageBase'], 16))\n",
    "    image_sizes.append(int(entry['metadata']['application']['pe']['optionalHeader']['sizeOfImage'], 16))\n",
    "\n",
    "    \n",
    "        # It is possible sections do not exist!\n",
    "#    if 'optionalHeader' not in entry['metadata']['application']['pe']:\n",
    "#        #print(sha)\n",
    "#        rva_difference.append(0)\n",
    "#    else:\n",
    "#        if 'dataDirectories' not in entry['metadata']['application']['pe']['optionalHeader']:\n",
    "#            if 'numberOfRvaAndSizes' not in entry['metadata']['application']['pe']['optionalHeader']:\n",
    "#                rva_difference.append(0)\n",
    "#            else:\n",
    "#                rva_difference.append(int(entry['metadata']['application']['pe']['optionalHeader']['numberOfRvaAndSizes'], 0))\n",
    "#        else:\n",
    "#            rva_difference.append(int(entry['metadata']['application']['pe']['optionalHeader']['numberOfRvaAndSizes'], 0) - \n",
    "#                              len(entry['metadata']['application']['pe']['optionalHeader']\n",
    "#                                      ['dataDirectories']['dataDirectory_list']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43784/43784 [00:00<00:00, 1407380.15it/s]\n",
      "100%|██████████| 43784/43784 [00:00<00:00, 1305064.15it/s]\n"
     ]
    }
   ],
   "source": [
    "sections = pandas.DataFrame(data={'sha1': shas})\n",
    "\n",
    "sections['group'] = [sha_group[sha1] for sha1 in tqdm(sections.sha1.values)]\n",
    "sections['label'] = [sha_label[sha1] for sha1 in tqdm(sections.sha1.values)]\n",
    "sections['entropy'] = entropies\n",
    "sections['imports'] = imports\n",
    "sections['imports_count'] = imports_count\n",
    "sections['apis'] = apis\n",
    "sections['apis_count'] = apis_count\n",
    "sections['has_children'] = has_children\n",
    "sections['children_entropies_mean'] = children_entropies_mean\n",
    "sections['children_entropies_std'] = children_entropies_std\n",
    "sections['section_names'] = section_names\n",
    "\n",
    "sections['text_entropy'] = text_entropy\n",
    "sections['data_entropy'] = data_entropy\n",
    "sections['rdata_entropy'] = rdata_entropy\n",
    "sections['buildid_entropy'] = buildid_entropy\n",
    "sections['pdata_entropy'] = pdata_entropy\n",
    "sections['xdata_entropy'] = xdata_entropy\n",
    "sections['bss_entropy'] = bss_entropy\n",
    "sections['edata_entropy'] = edata_entropy\n",
    "sections['idata_entropy'] = idata_entropy\n",
    "sections['reloc_entropy'] = reloc_entropy\n",
    "sections['no_name_entropy_mean'] = no_name_entropy_mean\n",
    "sections['no_name_entropy_std'] = no_name_entropy_std\n",
    "sections['other_entropy_mean'] = other_entropy_mean\n",
    "sections['other_entropy_std'] = other_entropy_std\n",
    "sections['tags'] = tags\n",
    "sections['checksum_valid'] = checksum_valid\n",
    "sections['entry_points_addresses'] = entry_points_addresses\n",
    "sections['image_bases'] = image_bases\n",
    "sections['image_sizes'] = image_sizes\n",
    "sections['children_tags'] = children_tags\n",
    "\n",
    "sections['children_sections_entropies_mean'] = children_sections_entropies_mean\n",
    "sections['children_sections_entropies_std'] = children_sections_entropies_std\n",
    "\n",
    "#sections['rva_difference'] = rva_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sections.to_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-real-27.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 1931.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sha1  \\\n",
      "0  00c9a60ecc3d1b7c5647f4b6c89fc41eeaec7e2d   \n",
      "1  0d6d94a2a4d5f64e2bfbd188326e8320f608100a   \n",
      "2  0acca648daf940e57b1daea9296aa11be7b4480a   \n",
      "3  0b331dd33f4d70780b0be554c7cdd7ba029ee000   \n",
      "4  0ded3d2726d2e397b6c1fdc26303c0818cc5fc2d   \n",
      "5  0d17380c8c6e86fbfd1a3cc381e3bf898fa16700   \n",
      "\n",
      "                                             imports  \\\n",
      "0                                               None   \n",
      "1                       [KERNEL32.DLL, MSVBVM60.DLL]   \n",
      "2  [advapi32.dll, kernel32.dll, oleaut32.dll, use...   \n",
      "3                         [kernel32.dll, user32.dll]   \n",
      "4  [SHLWAPI.dll, api-ms-win-core-com-l1-1-1.dll, ...   \n",
      "5  [ADVAPI32.dll, GDI32.dll, KERNEL32.dll, USER32...   \n",
      "\n",
      "                                                apis  \n",
      "0                                               None  \n",
      "1             [LoadLibraryA, GetProcAddress, 0x0064]  \n",
      "2  [RegQueryValueExA, LoadLibraryA, GetProcAddres...  \n",
      "3  [GetModuleHandleA, LoadLibraryA, GetProcAddres...  \n",
      "4  [SHSetThreadRef, CoTaskMemAlloc, CoAddRefServe...  \n",
      "5  [RegQueryValueExA, RegEnumKeyExA, RegCreateKey...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group_simple.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}\n",
    "shas = list(sha_label.keys())\n",
    "\n",
    "imports = []\n",
    "apis = []\n",
    "for sha in tqdm(shas):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports-simple/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    \n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    # It is possible sections do not exist!\n",
    "    if 'imports' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        imports.append(None)\n",
    "        apis.append(None)\n",
    "    else:\n",
    "        imports.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']])\n",
    "        apiss = [sec.get('api_list', 'NO_API_LIST')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']]\n",
    "        apis.append(list(itertools.chain.from_iterable(apiss)))\n",
    "\n",
    "sections = pandas.DataFrame(data={'sha1': shas, \n",
    "                                  'imports': imports,\n",
    "                                  'apis': apis\n",
    "                                 })\n",
    "sections = sections[['sha1', 'imports', 'apis']]\n",
    "print(sections)\n",
    "sections.to_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-imports-apis.csv', sep=',')\n",
    "#print(sections)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tc(sha):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    \n",
    "    entropies.append(entry['info']['file']['entropy'])\n",
    "\n",
    "    # It is possible sections do not exist!\n",
    "    if 'sections' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        section_names.append(None)\n",
    "    else:\n",
    "        section_names.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['sections']['section_list']])\n",
    "\n",
    "    children_entries = report['coreReport']['entries']['entry_list'][1:]\n",
    "    if children_entries:\n",
    "        entropy_list = [float(child_entry['info']['file']['entropy']) for child_entry in children_entries]\n",
    "        #print (entropy_list)\n",
    "        children_entropies.append(reduce(lambda x, y: x + y, entropy_list) / float(len(entropy_list)))            \n",
    "    else:\n",
    "        children_entropies.append(None)\n",
    "        \n",
    "    # It is possible sections do not exist!\n",
    "    if 'imports' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        imports.append(None)\n",
    "        apis.append(None)\n",
    "    else:\n",
    "        imports.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']])\n",
    "        apiss = [sec.get('api_list', 'NO_API_LIST')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']]\n",
    "        apis.append(list(itertools.chain.from_iterable(apiss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tc_simple(sha, imports, apis):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports-simple/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    \n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    # It is possible sections do not exist!\n",
    "    if 'imports' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        imports.append(None)\n",
    "        apis.append(None)\n",
    "    else:\n",
    "        imports.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']])\n",
    "        apiss = [sec.get('api_list', 'NO_API_LIST')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']]\n",
    "        apis.append(list(itertools.chain.from_iterable(apiss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None] [] []\n"
     ]
    }
   ],
   "source": [
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group_simple.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}\n",
    "shas = list(sha_label.keys())\n",
    "\n",
    "imports = []\n",
    "apis = []\n",
    "    \n",
    "results = Parallel(n_jobs=4)(delayed(parse_tc_simple)(sha, imports, apis) for sha in shas)\n",
    "print(results, imports, apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children_entropies</th>\n",
       "      <th>entropy</th>\n",
       "      <th>section_names</th>\n",
       "      <th>sha1</th>\n",
       "      <th>group</th>\n",
       "      <th>label</th>\n",
       "      <th>imports</th>\n",
       "      <th>apis</th>\n",
       "      <th>rva_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.990462</td>\n",
       "      <td>6.3537541159190685</td>\n",
       "      <td>[.text, .rdata, .data, .rsrc, .reloc]</td>\n",
       "      <td>640a57e82595478a222f2ba8cb7e7538c028377e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADVAPI32.dll, KERNEL32.dll, USER32.dll]</td>\n",
       "      <td>[RegQueryValueExW, RegCloseKey, RegOpenKeyExW,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.064054</td>\n",
       "      <td>7.6254575401548452</td>\n",
       "      <td>[pcs1, pcs2, pcs3, pcs4, pcs5, pcs6, pcs7]</td>\n",
       "      <td>45d05302c4041b7e4c53557f41f027be4ddaf2c8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[KERNEL32.DLL]</td>\n",
       "      <td>[LoadLibraryA, GetProcAddress, GlobalAlloc, Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.9283141446873628</td>\n",
       "      <td>[.text, .rdata, .data, .pdata, .didat, .tls, ....</td>\n",
       "      <td>5ef8d15a86d2f591110ce156b7eb16c9849d0e05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[BiWinrt.dll, PROPSYS.dll, RPCRT4.dll, api-ms-...</td>\n",
       "      <td>[BiRtDeleteEventForApp, BiRtCreateEventForApp,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.596947</td>\n",
       "      <td>1.9956394908434671</td>\n",
       "      <td>[.text, .data, .idata, .didat, .rsrc, .reloc]</td>\n",
       "      <td>f646dd86396a3b708eb32c3e476fb1dcd42b02bb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[GDI32.dll, SETUPAPI.dll, USER32.dll, api-ms-w...</td>\n",
       "      <td>[CreateDIBSection, SelectObject, CreateCompati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.824051</td>\n",
       "      <td>7.4573441889764824</td>\n",
       "      <td>[.text, .rsrc, .reloc]</td>\n",
       "      <td>23271ba9263638f3e966f198afbdb0ff82488659</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[kernel32.dll]</td>\n",
       "      <td>[LoadLibraryA, GetProcAddress, VirtualAlloc, V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children_entropies             entropy  \\\n",
       "0            4.990462  6.3537541159190685   \n",
       "1            3.064054  7.6254575401548452   \n",
       "2                 NaN  5.9283141446873628   \n",
       "3            1.596947  1.9956394908434671   \n",
       "4            2.824051  7.4573441889764824   \n",
       "\n",
       "                                       section_names  \\\n",
       "0              [.text, .rdata, .data, .rsrc, .reloc]   \n",
       "1         [pcs1, pcs2, pcs3, pcs4, pcs5, pcs6, pcs7]   \n",
       "2  [.text, .rdata, .data, .pdata, .didat, .tls, ....   \n",
       "3      [.text, .data, .idata, .didat, .rsrc, .reloc]   \n",
       "4                             [.text, .rsrc, .reloc]   \n",
       "\n",
       "                                       sha1  group  label  \\\n",
       "0  640a57e82595478a222f2ba8cb7e7538c028377e      0      0   \n",
       "1  45d05302c4041b7e4c53557f41f027be4ddaf2c8      3      1   \n",
       "2  5ef8d15a86d2f591110ce156b7eb16c9849d0e05      0      0   \n",
       "3  f646dd86396a3b708eb32c3e476fb1dcd42b02bb      0      0   \n",
       "4  23271ba9263638f3e966f198afbdb0ff82488659      3      1   \n",
       "\n",
       "                                             imports  \\\n",
       "0           [ADVAPI32.dll, KERNEL32.dll, USER32.dll]   \n",
       "1                                     [KERNEL32.DLL]   \n",
       "2  [BiWinrt.dll, PROPSYS.dll, RPCRT4.dll, api-ms-...   \n",
       "3  [GDI32.dll, SETUPAPI.dll, USER32.dll, api-ms-w...   \n",
       "4                                     [kernel32.dll]   \n",
       "\n",
       "                                                apis  rva_difference  \n",
       "0  [RegQueryValueExW, RegCloseKey, RegOpenKeyExW,...               0  \n",
       "1  [LoadLibraryA, GetProcAddress, GlobalAlloc, Ex...               0  \n",
       "2  [BiRtDeleteEventForApp, BiRtCreateEventForApp,...               0  \n",
       "3  [CreateDIBSection, SelectObject, CreateCompati...               0  \n",
       "4  [LoadLibraryA, GetProcAddress, VirtualAlloc, V...               0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[sections['rva_difference'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 865.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rva_difference                                      sha1\n",
      "0               0  00c9a60ecc3d1b7c5647f4b6c89fc41eeaec7e2d\n",
      "1               0  0d6d94a2a4d5f64e2bfbd188326e8320f608100a\n",
      "2               0  0acca648daf940e57b1daea9296aa11be7b4480a\n",
      "3               0  0b331dd33f4d70780b0be554c7cdd7ba029ee000\n",
      "4               0  0ded3d2726d2e397b6c1fdc26303c0818cc5fc2d\n",
      "5               0  0d17380c8c6e86fbfd1a3cc381e3bf898fa16700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group_simple.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}\n",
    "shas = list(sha_label.keys())\n",
    "\n",
    "rva_difference = []\n",
    "for sha in tqdm(shas):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports-simple/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    \n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    # It is possible sections do not exist!\n",
    "    if 'optionalHeader' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        rva_difference.append(0)\n",
    "    else:\n",
    "        rva_difference.append(int(entry['metadata']['application']['pe']['optionalHeader']['numberOfRvaAndSizes'], 0) - \n",
    "                              len(entry['metadata']['application']['pe']['optionalHeader']\n",
    "                                      ['dataDirectories']['dataDirectory_list']))\n",
    "\n",
    "                              \n",
    "sections = pandas.DataFrame(data={'sha1': shas, \n",
    "                                  'rva_difference': rva_difference\n",
    "                                 })\n",
    "print(sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sections.to_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-5.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43784/43784 [03:08<00:00, 232.35it/s]\n",
      "100%|██████████| 43784/43784 [00:00<00:00, 1058994.46it/s]\n",
      "100%|██████████| 43784/43784 [00:00<00:00, 1144402.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os.path import dirname\n",
    "import sys\n",
    "import ujson\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas\n",
    "from multiprocessing import Value, Lock, RawValue\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import numpy\n",
    "\n",
    "shas, groups = zip(*[x.split(',') for x in open('samples_by_packer_group.txt').read().splitlines()])\n",
    "sha_group = {a: int(b) for a, b in zip(shas, groups)}\n",
    "sha_label = {a: 1 if int(b) else 0 for a, b in zip(shas, groups)}\n",
    "shas = list(sha_label.keys())\n",
    "# Extracting section names from TC report\n",
    "section_names = []\n",
    "entropies = []\n",
    "children_entropies = []\n",
    "imports = []\n",
    "apis = []\n",
    "rva_difference = []\n",
    "for sha in tqdm(shas):\n",
    "    report = ujson.load(\n",
    "        open(\"/home/matko/Desktop/mozgalo/reports/{}/{}.json\".format(sha[-2:], sha), 'rb')\n",
    "    )\n",
    "    entry = report['coreReport']['entries']['entry_list'][0]\n",
    "    \n",
    "    entropies.append(entry['info']['file']['entropy'])\n",
    "\n",
    "    # It is possible sections do not exist!\n",
    "    if 'sections' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        section_names.append(None)\n",
    "    else:\n",
    "        section_names.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['sections']['section_list']])\n",
    "\n",
    "    children_entries = report['coreReport']['entries']['entry_list'][1:]\n",
    "    if children_entries:\n",
    "        entropy_list = [float(child_entry['info']['file']['entropy']) for child_entry in children_entries]\n",
    "        #print (entropy_list)\n",
    "        #children_entropies.append(reduce(lambda x, y: x + y, entropy_list) / float(len(entropy_list)))   \n",
    "        children_entropies.append(numpy.mean(numpy.array(entropy_list)))\n",
    "    else:\n",
    "        children_entropies.append(None)\n",
    "        \n",
    "    # It is possible sections do not exist!\n",
    "    if 'imports' not in entry['metadata']['application']['pe']:\n",
    "        #print(sha)\n",
    "        imports.append(None)\n",
    "        apis.append(None)\n",
    "    else:\n",
    "        imports.append(\n",
    "            [sec.get('name', 'NO_NAME')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']])\n",
    "        apiss = [sec.get('api_list', 'NO_API_LIST')\n",
    "             for sec in entry['metadata']['application']['pe']['imports']['import_list']]\n",
    "        apis.append(list(itertools.chain.from_iterable(apiss)))\n",
    "    \n",
    "\n",
    "sections = pandas.DataFrame(data={'sha1': shas, \n",
    "                                  'section_names': section_names,\n",
    "                                  'entropy': entropies,\n",
    "                                  'children_entropies': children_entropies\n",
    "                                 })\n",
    "sections['group'] = [sha_group[sha1] for sha1 in tqdm(sections.sha1.values)]\n",
    "sections['label'] = [sha_label[sha1] for sha1 in tqdm(sections.sha1.values)]\n",
    "sections['imports'] = imports\n",
    "sections['apis'] = apis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections.to_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-7.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "need to escape, but no escapechar set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-65f9c158524f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-7-test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1524\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m   1778\u001b[0m                                         quoting=self.quoting)\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: need to escape, but no escapechar set"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "sections.to_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-7-test.csv', index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 3, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-11d8bf7ad360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-imports-apis.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 3, saw 7\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "pandas.read_csv('/home/matko/Desktop/mozgalo/reports-csv-simple/mycsv-imports-apis.csv', skipinitialspace = True, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43784/43784 [00:06<00:00, 6921.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".rsrc     \t38018\n",
      ".text     \t18590\n",
      ".data     \t15509\n",
      ".reloc    \t15290\n",
      ".rdata    \t14513\n",
      "NO_NAME   \t13526\n",
      "UPX1      \t 8612\n",
      "UPX0      \t 8612\n",
      ".idata    \t 4011\n",
      ".pdata    \t 3997\n",
      "pec1      \t 2980\n",
      ".imports  \t 2834\n",
      "pec2      \t 2171\n",
      "UPX2      \t 2013\n",
      ".adata    \t 1648\n",
      ".tls      \t 1576\n",
      ".pec      \t 1403\n",
      "pec3      \t 1337\n",
      "INIT      \t 1190\n",
      ".edata    \t 1126\n",
      "CODE      \t 1102\n",
      "BSS       \t 1006\n",
      "DATA      \t  977\n",
      ".gfids    \t  915\n",
      "PAGE      \t  905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter()\n",
    "for names in tqdm(sections.section_names.values):\n",
    "    cnt += Counter(names)\n",
    "num_used = 25\n",
    "for a, b in cnt.most_common()[:num_used]:\n",
    "    print(\"{:10}\\t{:5}\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used = [x[0] for x in cnt.most_common()[:num_used]]\n",
    "name_ind = {name: i for i, name in enumerate(most_used)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43784/43784 [00:00<00:00, 164513.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "data = numpy.zeros((sections.shape[0], num_used + 1))\n",
    "for i, names in enumerate(tqdm(sections.section_names.values)):\n",
    "    for name in names:\n",
    "        if name in name_ind:\n",
    "            data[i, name_ind[name]] += 1\n",
    "        else:\n",
    "            data[i, num_used] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
